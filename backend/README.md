# ğŸš€ ODRL Policy Generator

**Multi-Agent LLM System for Automatic ODRL Policy Generation**

Transform natural language descriptions into valid ODRL (Open Digital Rights Language) policies using a sophisticated multi-agent architecture with flexible LLM support.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-green.svg)](https://python.langchain.com/)

---

## âœ¨ Features

- ğŸ¤– **Multi-Agent Architecture**: Four specialized agents (Parser, Reasoner, Generator, Validator)
- ğŸ”„ **Flexible LLM Support**: Works with Ollama, Groq, OpenAI, Claude, and custom models
- ğŸ¨ **Modern GUI**: Interactive dashboard with real-time processing visualization
- âš¡ **Performance Metrics**: Track timing, tokens, and efficiency
- ğŸŒ™ **Dark Mode**: Professional aesthetic for demos and presentations
- ğŸ“Š **SHACL Validation**: Automatic validation against ODRL specifications
- ğŸ”§ **Zero-Configuration**: Auto-detects available LLM providers
- ğŸ’° **FREE Options**: Full support for Groq's free tier

---

## ğŸ¯ Quick Start (2 Minutes)

### Prerequisites
- Python 3.8+
- A FREE Groq API key ([Get one here](https://console.groq.com/keys))

### Installation
```bash
# 1. Clone repository
git clone https://github.com/your-username/odrl-generator.git
cd odrl-generator

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure (add your Groq key)
cd backend
cp .env.example .env
nano .env  # Add: GROQ_API_KEY=gsk_your_key_here

# 4. Run backend
uvicorn main:app --reload

# 5. Open GUI
# Open frontend/index.html in your browser
# Visit: http://localhost:8000
```

**That's it!** ğŸ‰ You should see "Backend Connected" and be ready to generate policies.

---

## ğŸ“Š Architecture

```
Natural Language â†’ Parser â†’ Reasoner â†’ Generator â†’ Validator â†’ ODRL Policy
                     â†“         â†“          â†“           â†“
                   Agent 1   Agent 2   Agent 3    Agent 4
                   (LLM)     (LLM)     (LLM)      (LLM+SHACL)
```

### Agent Pipeline

1. **Parser**: Extracts structured information from text
2. **Reasoner**: Applies logical reasoning and policy rules
3. **Generator**: Creates valid ODRL JSON-LD policy
4. **Validator**: Validates against SHACL shapes and standards

---

## ğŸ® Usage Examples

### Example 1: Document Policy
**Input:**
```
Users can read and print the document but cannot modify or distribute it.
The policy expires on December 31, 2025.
```

**Output:**
```json
{
  "@context": "http://www.w3.org/ns/odrl.jsonld",
  "@type": "Offer",
  "permission": [
    {
      "action": "read",
      "constraint": [
        {"leftOperand": "dateTime", "operator": "lteq", "rightOperand": "2025-12-31"}
      ]
    }
  ],
  "prohibition": [
    {"action": "modify"},
    {"action": "distribute"}
  ]
}
```

### Example 2: Academic Dataset
**Input:**
```
Researchers can download and analyze the dataset for non-commercial research.
Attribution is required. Commercial use is prohibited.
```

**Output:** Complete ODRL policy with permissions, duties, and prohibitions.

---

## ğŸ”§ Configuration

### Option 1: FREE Cloud (Groq)
```bash
# backend/.env
ENABLE_GROQ=true
GROQ_API_KEY=gsk_your_key_here
DEFAULT_MODEL=groq:llama-3.1-70b-versatile
```

### Option 2: Local Ollama
```bash
# backend/.env
ENABLE_OLLAMA=true
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=ollama:llama3.1:8b
```

### Option 3: Research Server
```bash
# backend/.env
ENABLE_FITS=true
FITS_SERVER_URL=http://your-server:11434
DEFAULT_MODEL=ollama:llama3.1:70b
```

### Option 4: Multiple Providers
```bash
# backend/.env
ENABLE_OLLAMA=true
ENABLE_GROQ=true
ENABLE_OPENAI=true
# Users can switch in GUI!
```

See [SETUP.md](SETUP.md) for complete configuration guide.

---

## ğŸ“ Project Structure

```
odrl-generator/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI server
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ llm_router.py       # Smart LLM provider router
â”‚   â”‚   â”œâ”€â”€ text_parser/        # Agent 1: Text parsing
â”‚   â”‚   â”œâ”€â”€ reasoner/           # Agent 2: Reasoning
â”‚   â”‚   â”œâ”€â”€ generator/          # Agent 3: ODRL generation
â”‚   â”‚   â””â”€â”€ validator/          # Agent 4: SHACL validation
â”‚   â”œâ”€â”€ .env.example            # Configuration template
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # GUI interface
â”‚   â””â”€â”€ components/             # React components
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SETUP.md                # Detailed setup guide
â”‚   â”œâ”€â”€ USER_GUIDE.md           # User documentation
â”‚   â””â”€â”€ API.md                  # API documentation
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ examples/                   # Example policies
â””â”€â”€ README.md                   # This file
```

---

## ğŸ› ï¸ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/available-providers` | GET | List all available LLM providers |
| `/api/parse` | POST | Parse natural language text |
| `/api/reason` | POST | Apply reasoning to parsed data |
| `/api/generate` | POST | Generate ODRL policy |
| `/api/validate` | POST | Validate ODRL with SHACL |

Interactive API docs: `http://localhost:8000/docs`

---

## ğŸ“ For Researchers

### Citation
```bibtex
@software{odrl_generator_2025,
  title={ODRL Policy Generator: Multi-Agent LLM System},
  author={Your Name},
  year={2025},
  url={https://github.com/your-username/odrl-generator}
}
```

### Key Features for Papers
- âœ… Multi-agent architecture demonstration
- âœ… LLM flexibility (local, cloud, custom)
- âœ… Performance metrics collection
- âœ… SHACL validation integration
- âœ… Real-world policy examples

---

## ğŸ“Š Performance

| Model | Parser | Reasoner | Generator | Validator | Total |
|-------|--------|----------|-----------|-----------|-------|
| Groq Llama 3.1 70B | ~500ms | ~600ms | ~700ms | ~400ms | ~2.2s |
| Local Llama 3.1 8B | ~2s | ~2.5s | ~3s | ~1s | ~8.5s |
| GPT-4 Turbo | ~800ms | ~900ms | ~1s | ~500ms | ~3.2s |

*Tested on example "Document Policy" with default settings*

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

---

## ğŸŒŸ Acknowledgments

- Built with [LangChain](https://python.langchain.com/)
- ODRL specification: [W3C ODRL](https://www.w3.org/TR/odrl-model/)
- SHACL validation: [pySHACL](https://github.com/RDFLib/pySHACL)
- LLM providers: [Ollama](https://ollama.com/), [Groq](https://groq.com/), [OpenAI](https://openai.com/)

---

## ğŸ“ Support

- ğŸ“– [User Guide](docs/USER_GUIDE.md)
- ğŸ”§ [Setup Guide](docs/SETUP.md)
- ğŸ› [Issue Tracker](https://github.com/your-username/odrl-generator/issues)
- ğŸ’¬ [Discussions](https://github.com/your-username/odrl-generator/discussions)

---

## ğŸ—ºï¸ Roadmap

- [x] Multi-agent architecture
- [x] Multiple LLM provider support
- [x] Interactive GUI with metrics
- [x] SHACL validation
- [ ] Batch processing
- [ ] Model comparison mode
- [ ] Confidence scoring
- [ ] Policy templates library
- [ ] REST API authentication
- [ ] Docker deployment

---

## â­ Star History

If this project helped you, please consider giving it a star! â­

---

**Made with â¤ï¸ for the ODRL and Semantic Web community**